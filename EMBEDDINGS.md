# ğŸ”¢ BÃºsqueda SemÃ¡ntica con Embeddings

## âœ… ActualizaciÃ³n Implementada

El sistema ahora usa **embeddings de Cohere** para bÃºsqueda semÃ¡ntica en lugar de bÃºsqueda simple. Esto mejora DRÃSTICAMENTE la calidad de los candidatos antes del Rerank.

## ğŸ¯ Â¿QuÃ© son los Embeddings?

Los embeddings son representaciones numÃ©ricas (vectores) de texto que capturan su **significado semÃ¡ntico**.

### Ejemplo:
```
"Â¿CuÃ¡l es el plazo para apelar?" 
â†’ [0.234, -0.891, 0.456, ..., 0.123]  (1024 dimensiones)

"El plazo de apelaciÃ³n es de 10 dÃ­as"
â†’ [0.221, -0.875, 0.443, ..., 0.119]  (1024 dimensiones)

"Receta de pizza napolitana"
â†’ [-0.567, 0.234, -0.891, ..., -0.445]  (1024 dimensiones)
```

Los primeros dos vectores son **muy similares** (tema legal, apelaciones).
El tercero es **muy diferente** (tema cocina).

## ğŸ”„ Flujo Actualizado

### ANTES (BÃºsqueda Simple):
```
Usuario: "Â¿CuÃ¡l es el plazo para apelar?"
    â†“
[Paso 1] Retorna TODOS los documentos (sin filtro real)
    â†“
[Paso 2] Rerank ordena documentos
    â†“
[Paso 3] Genera respuesta
```

### AHORA (BÃºsqueda SemÃ¡ntica):
```
Usuario: "Â¿CuÃ¡l es el plazo para apelar?"
    â†“
[Paso 1a] Genera embedding de la query
[Paso 1b] Calcula similaridad con todos los documentos
[Paso 1c] Retorna TOP 20 mÃ¡s similares semÃ¡nticamente
    â†“
[Paso 2] Rerank afina el orden de esos 20
    â†“
[Paso 3] Genera respuesta con TOP 5 finales
```

## ğŸ§® Similaridad Coseno

La similaridad entre dos vectores se calcula con el **cosine similarity**:

```python
similarity = cos(Î¸) = (A Â· B) / (||A|| Ã— ||B||)
```

- **1.0**: Vectores idÃ©nticos (mÃ¡xima similaridad)
- **0.0**: Vectores perpendiculares (sin relaciÃ³n)
- **-1.0**: Vectores opuestos

### Ejemplo Real:
```python
query_embedding = [0.5, 0.8, 0.2]
doc1_embedding  = [0.6, 0.7, 0.3]  # Similaridad: 0.95 âœ… MUY SIMILAR
doc2_embedding  = [0.1, 0.2, 0.9]  # Similaridad: 0.42 âš ï¸ POCO SIMILAR
```

## ğŸ”§ ImplementaciÃ³n TÃ©cnica

### 1. Al cargar documentos:
```python
# Se generan embeddings de TODOS los documentos
response = client.embed(
    texts=[doc1.content, doc2.content, ...],
    model="embed-multilingual-v3.0",
    input_type="search_document"  # â† Importante: tipo documento
)

# Se almacenan como matriz NumPy
document_embeddings = np.array(response.embeddings.float)
# Shape: (num_documentos, 1024)
```

### 2. Al hacer una query:
```python
# Generar embedding de la query
query_response = client.embed(
    texts=[query],
    model="embed-multilingual-v3.0",
    input_type="search_query"  # â† Importante: tipo query
)
query_embedding = np.array(query_response.embeddings.float[0])
# Shape: (1024,)

# Calcular similaridades
similarities = cosine_similarity(query_embedding, document_embeddings)
# Shape: (num_documentos,)

# Obtener top N
top_indices = np.argsort(similarities)[::-1][:20]
candidates = [documents[i] for i in top_indices]
```

## ğŸ“ Modelo de Embeddings

**`embed-multilingual-v3.0`** (usado en el proyecto):
- âœ… **1024 dimensiones**
- âœ… **100+ idiomas** (excelente para espaÃ±ol)
- âœ… Optimizado para bÃºsqueda semÃ¡ntica
- âœ… Alta calidad en contenido jurÃ­dico

Alternativas:
- `embed-english-v3.0`: Solo inglÃ©s (mÃ¡s rÃ¡pido)
- `embed-multilingual-light-v3.0`: MÃ¡s rÃ¡pido, menos preciso

## ğŸ“Š Ventajas de Embeddings sobre BÃºsqueda Simple

| Aspecto | BÃºsqueda Simple | Embeddings |
|---------|----------------|------------|
| **SinÃ³nimos** | âŒ No detecta | âœ… "plazo" = "tÃ©rmino" |
| **Contexto** | âŒ Ignora | âœ… Entiende tema |
| **MultilingÃ¼e** | âŒ Limitado | âœ… Excelente |
| **Typos** | âŒ Falla | âœ… Tolera errores |
| **SemÃ¡ntica** | âŒ No | âœ… Captura significado |

### Ejemplo PrÃ¡ctico:

**Query**: "Â¿CuÃ¡nto tiempo tengo para impugnar una resoluciÃ³n?"

**BÃºsqueda Simple**: 
- BuscarÃ­a palabras: "tiempo", "impugnar", "resoluciÃ³n"
- PodrÃ­a NO encontrar documentos que usen "plazo", "apelar", "sentencia"

**Embeddings**:
- Entiende que "impugnar" â‰ˆ "apelar"
- Entiende que "tiempo" â‰ˆ "plazo"
- Entiende que "resoluciÃ³n" â‰ˆ "sentencia"
- âœ… Encuentra documentos relevantes aunque usen palabras diferentes

## ğŸ’° Impacto en Costos

### Costos Adicionales:
- **Generar embeddings al cargar**: ~$0.0001 por documento
- **Embedding de query**: ~$0.0001 por consulta

Para 100 documentos:
- Carga inicial: ~$0.01 (una sola vez)
- Por consulta: ~$0.0001 (insignificante)

**Total por consulta completa**:
- Embeddings: $0.0001
- Rerank: $0.002
- Generation: $0.02
- **Total**: ~$0.022 (prÃ¡cticamente igual que antes)

## ğŸ§ª CÃ³mo Probar la Mejora

### Test Comparativo:

```python
# Ejecutar con embeddings (actual)
python main.py

# Observar los scores de similaridad en Paso 1:
#   â†’ #1 - Score: 0.8523 - plazos_legales.md  âœ… Alta relevancia
#   â†’ #2 - Score: 0.7234 - recursos_judiciales.md
```

NotarÃ¡s que:
1. **Los candidatos son mÃ¡s relevantes** desde el Paso 1
2. **Rerank tiene mejor material** para ordenar
3. **La respuesta final es mÃ¡s precisa**

## ğŸ” Ver los Embeddings en AcciÃ³n

Agrega este cÃ³digo en `ejemplos_avanzados.py`:

```python
def ejemplo_visualizar_embeddings():
    """Ver cÃ³mo funcionan los embeddings"""
    from rag_system import LegalRAGSystem
    import numpy as np
    
    rag = LegalRAGSystem(api_key=os.getenv("COHERE_API_KEY"))
    rag.load_documents_from_folder("data/legal_docs")
    
    # Consultas de prueba
    queries = [
        "Â¿CuÃ¡l es el plazo para apelar?",
        "Â¿QuÃ© es un recurso de casaciÃ³n?",
        "Receta de pizza napolitana"  # â† NO relevante
    ]
    
    for query in queries:
        print(f"\nQuery: {query}")
        
        # Generar embedding de query
        query_emb = rag.client.embed(
            texts=[query],
            model="embed-multilingual-v3.0",
            input_type="search_query"
        ).embeddings.float[0]
        
        # Calcular similaridades
        sims = rag._cosine_similarity(
            np.array(query_emb), 
            rag.document_embeddings
        )
        
        # Mostrar resultados
        for i, doc in enumerate(rag.documents):
            print(f"  {doc.metadata['source']}: {sims[i]:.4f}")
```

## ğŸ“š PrÃ³ximos Pasos

Para optimizar aÃºn mÃ¡s:

1. **Cachear embeddings**:
   ```python
   # Guardar embeddings en disco
   np.save('doc_embeddings.npy', document_embeddings)
   
   # Cargar en vez de regenerar
   document_embeddings = np.load('doc_embeddings.npy')
   ```

2. **Usar base de datos vectorial** (ChromaDB, Pinecone):
   - Almacenamiento persistente
   - BÃºsqueda optimizada
   - Escalable a millones de documentos

3. **Chunking + Embeddings**:
   - Dividir documentos largos en chunks
   - Cada chunk tiene su embedding
   - Mayor precisiÃ³n en la bÃºsqueda

## ğŸ¯ Resumen

âœ… **Implementado**: BÃºsqueda semÃ¡ntica con `embed-multilingual-v3.0`
âœ… **Ventaja**: Encuentra documentos relevantes por significado, no solo por palabras
âœ… **Costo**: PrÃ¡cticamente igual (~$0.0001 adicional)
âœ… **Calidad**: MUCHO mejor que bÃºsqueda simple
âœ… **Listo**: Ya funciona en el sistema, solo ejecuta `python main.py`

Â¡Ahora tienes un sistema RAG de producciÃ³n con bÃºsqueda semÃ¡ntica! ğŸš€
