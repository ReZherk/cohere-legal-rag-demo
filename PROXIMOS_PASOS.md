# Pr√≥ximos Pasos y Mejoras üöÄ

## Para el Proyecto Grande (Producci√≥n)

### 1. Sistema de Embeddings y B√∫squeda Vectorial
Actualmente usamos b√∫squeda simple. Para producci√≥n, implementa:

```python
# Opci√≥n A: ChromaDB (local, f√°cil)
import chromadb

client = chromadb.Client()
collection = client.create_collection("legal_docs")

# Agregar documentos con embeddings autom√°ticos
collection.add(
    documents=[doc.content for doc in documents],
    ids=[str(i) for i in range(len(documents))]
)

# B√∫squeda sem√°ntica
results = collection.query(
    query_texts=["¬øCu√°l es el plazo para apelar?"],
    n_results=20
)
```

```python
# Opci√≥n B: Cohere Embed (mejor calidad)
import cohere

co = cohere.Client(api_key)

# Generar embeddings
embeddings = co.embed(
    texts=[doc.content for doc in documents],
    model="embed-multilingual-v3.0"  # Soporte espa√±ol
).embeddings

# Almacenar en Pinecone, Weaviate, o Qdrant
```

### 2. Chunking Inteligente
Divide documentos largos en chunks:

```python
from langchain.text_splitter import RecursiveCharacterTextSplitter

splitter = RecursiveCharacterTextSplitter(
    chunk_size=500,
    chunk_overlap=50,
    separators=["\n\n", "\n", ". ", " ", ""]
)

chunks = splitter.split_text(documento_largo)
```

### 3. Cach√© de Respuestas
Evita llamadas repetidas a la API:

```python
import hashlib
import json

class ResponseCache:
    def __init__(self):
        self.cache = {}
    
    def get_cache_key(self, query: str) -> str:
        return hashlib.md5(query.encode()).hexdigest()
    
    def get(self, query: str):
        return self.cache.get(self.get_cache_key(query))
    
    def set(self, query: str, response: str):
        self.cache[self.get_cache_key(query)] = response
```

### 4. API REST con FastAPI
Exp√≥n tu sistema como API:

```python
from fastapi import FastAPI
from pydantic import BaseModel

app = FastAPI()

class Query(BaseModel):
    question: str
    top_k: int = 5

@app.post("/query")
async def query_endpoint(query: Query):
    resultado = rag.query(query.question, top_k=query.top_k)
    return resultado

# Ejecutar: uvicorn api:app --reload
```

### 5. Interfaz Web con Streamlit
```python
import streamlit as st

st.title("üèõÔ∏è Asistente Legal con IA")

query = st.text_input("Tu consulta legal:")

if st.button("Consultar"):
    with st.spinner("Procesando..."):
        resultado = rag.query(query)
        st.write(resultado['answer'])
        
        with st.expander("Ver documentos usados"):
            for doc in resultado['context_docs']:
                st.write(f"**Score:** {doc['score']}")
                st.write(doc['content'][:300])
```

### 6. Evaluaci√≥n y M√©tricas
Mide la calidad del sistema:

```python
# Crear dataset de evaluaci√≥n
eval_queries = [
    {
        "query": "¬øCu√°l es el plazo para apelar?",
        "expected_answer": "10 d√≠as h√°biles",
        "expected_docs": ["plazos_legales.md"]
    }
]

# Evaluar
for item in eval_queries:
    resultado = rag.query(item['query'])
    # Comparar con expected_answer
    # Verificar que expected_docs est√©n en context_docs
```

### 7. Manejo de Errores y Logging
```python
import logging

logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

try:
    resultado = rag.query(query)
except cohere.CohereError as e:
    logger.error(f"Error de Cohere: {e}")
    # Fallback o retry
```

### 8. Control de Costos
```python
class UsageTracker:
    def __init__(self):
        self.rerank_calls = 0
        self.generation_tokens = 0
    
    def log_rerank(self, num_docs: int):
        self.rerank_calls += 1
        # Estimar costo
    
    def log_generation(self, tokens: int):
        self.generation_tokens += tokens
```

### 9. Soporte Multiling√ºe
```python
# Detectar idioma
from langdetect import detect

def query_multilingue(query: str):
    idioma = detect(query)
    
    if idioma == 'en':
        prompt = "You are a legal assistant..."
    else:
        prompt = "Eres un asistente legal..."
```

### 10. Historial Conversacional
```python
class ConversationalRAG:
    def __init__(self):
        self.history = []
    
    def query_with_history(self, query: str):
        # Incluir historial en el prompt
        context = "\n".join([
            f"Usuario: {h['query']}\nAsistente: {h['answer']}"
            for h in self.history[-3:]  # √öltimas 3 interacciones
        ])
        
        # Generar respuesta con contexto conversacional
        # ...
        
        self.history.append({'query': query, 'answer': answer})
```

## Prioridad de Implementaci√≥n

### Fase 1 (Esencial)
1. ‚úÖ Sistema b√°sico RAG (ya tienes esto)
2. Embeddings + b√∫squeda vectorial (ChromaDB)
3. Chunking inteligente

### Fase 2 (Importante)
4. Cach√© de respuestas
5. Manejo de errores robusto
6. Logging y monitoreo

### Fase 3 (Nice-to-have)
7. API REST
8. Interfaz web
9. Evaluaci√≥n automatizada

### Fase 4 (Optimizaci√≥n)
10. Control de costos
11. Multiling√ºe
12. Historial conversacional

## Recursos Adicionales

- [Cohere RAG Guide](https://docs.cohere.com/docs/retrieval-augmented-generation-rag)
- [ChromaDB Docs](https://docs.trychroma.com/)
- [LangChain for RAG](https://python.langchain.com/docs/use_cases/question_answering/)
- [FastAPI Tutorial](https://fastapi.tiangolo.com/tutorial/)
